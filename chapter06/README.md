# 第6回 価値反復法の導入

## 報酬の導入
- 迷路課題であればゴールをした時に、ロボットの歩行であれば転ばずに歩けている間、毎ステップ**報酬**を与える
- 時刻tでもらえる報酬を即時報酬(immediate reward)$$R_t$$と呼ぶ -> $$R_t$$は課題に合わせて自分で設定
- 今後未来にわたって得られるであろう報酬の合計を報酬和$$G_t$$と呼ぶ -> $$G_t = R_{t+1} + R_{t+2} + R_{t+3} + ...$$
- **時間割引**を考慮した割引報酬和(discounted total reward)$$G_t$$ -> $$G_t = R_{t+1} + \gammaR_{t+2} + \gamma^2R_{t+3} + ...$$

## 価値の導入
- **行動価値**(action Value)と**状態価値**(state Value)を定義
- ゴールにたどり着いた瞬間Rt=1がもらえると設定
- 行動価値(S7にいる場合の例)
   - 行動価値は行動価値観数$$Q^{\pi}(s, a)$$と書かれる
   - もし、行動a=右だった場合、次のステップでゴールにたどりつき報酬Rt+1=1を手にすることができる
   - 上記を関数で表すと、$$Q^{\pi}(s=7, a=1)=1$$
   - もし、行動a=上だった場合、S7→S4→S7→S8と言うように2ステップ余分に時間がかかる
   - 上記を関数で表すと、$$Q^{\pi}(s=7, a=0)=\gamma^2\times1$$
   - ゴールにたどり着くために2ステップ多めにかかるのでその時間分だけ割り引いた報酬が得られる
- 状態価値
   = 状態sにおいて方策πにしたがって行動することで、その後将来にわたって得られることが期待される割引報酬和Gt
   - 状態sの状態価値観数を$$V^{\pi}(s)$$と書く
   - エージェントがS7にいる場合、右に移動すればゴールして報酬1を手に入れることができるので、$$V^{\pi}(s=7)=1$$
   - エージェントがS4にいる場合、S7に移動して右に移動すればゴールできるので、$$V^{\pi}(s=4)=\gamma\times1$$
   - S4にいる場合、V(s=4)=Rt+1 + γV(s=7)と表現することができる(πは省略/ゴール以外では報酬得られないのでRt+1=0)

## ベルマン方程式とマルコフ過程
- 価値観数を一般的な書き方で表した式(連載ページ参照)を**ベルマン方程式**と呼ぶ
- 状態sでの状態価値Vは、右辺が最も大きくなる行動aをとった時の期待値
- Rs,aは状態sで行動aをとった時に得られる即時報酬（ゴールにたどり着く場合以外は全部0）
- s(s,a)状態sで行動aを採用して移動した先の新たな状態sを示す
- ベルマン方程式が成り立つための前提条件 -> **マルコフ決定過程**(Markov Decision Process)
- 次のステップの状態st+1が現在の状態sと選択した行動aで確定するシステム

## 方策と価値の決定方法
- 行動価値、状態価値を決めるアルゴリズム -> Sarsa、Q学習、モンテカルロ法
- 方策は行動価値に基づいて決める